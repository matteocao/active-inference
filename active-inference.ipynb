{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f6754b-f000-4812-b774-82d793b407c6",
   "metadata": {},
   "source": [
    "# Active inference\n",
    "\n",
    "In this notebook we will go through the main ideas and concepts described in this book: https://mitpress.mit.edu/9780262045353/active-inference/\n",
    "\n",
    "The examples we will be revieing and implementing is well described here: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5167251/\n",
    "\n",
    "### Note\n",
    "\n",
    "In case you are using `venv`, you need to launch the following command before being able to run the jupyter notebook with the appropriate kernel:\n",
    "\n",
    "```\n",
    "python -m ipykernel install --user --name=<virtualenv_name>\n",
    "```\n",
    "\n",
    "Then, inside jupyter lab, check the top right to use the kernel named `<virtualenv_name>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abe4b3b9-d57a-410f-a475-a3dd5a0fb910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convenient scripts\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# imports\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from itertools import product\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d0ff6d-1a7b-4f79-bc84-a20d4e3b843a",
   "metadata": {},
   "source": [
    "## Mathematical modelling\n",
    "\n",
    "### Biological principles\n",
    "\n",
    "Sentient beings act on the world with one goal in mind: **survival**. Survival is maximised if **surprise is minimised**. What is surprise? It it the $log(1/P(y))$, where $y$ is a sensory input.\n",
    "\n",
    "How can $P$ be estiamted? The sentient being has to have a **model of the world** where its beliefs (modelled as priors) fit otgether to provide a posterior $P$ given the data observed.\n",
    "\n",
    "Furthermore, the way a senntient being act on the world and try to make sense of sensory inputs has also to have one goal inmind: minimise the prediction error of the model. This is equivalent to minimising the surprise.\n",
    "\n",
    "The last statement can be formulated in the language of bayesian inference: hence the name \"bayesian brain\".\n",
    "\n",
    "### Bayesian brain\n",
    "\n",
    "The brain is an inference machine according to the bayesian brain hypothesis. Given the sensory inputs $y$ and the hidden parameters $x$ that are used to make sense of the world, the goal of the brain is to compute $P(y|x)$, a.k.a. the posterior.\n",
    "\n",
    "Assuming that $P$ is the actual distribution of the world responses, the sentient being needs to find a good approximation of this distribution: this approximation will be denoited $Q$. How can we compute $Q$? By minimising free energy (see below).\n",
    "\n",
    "### Markov blanket\n",
    "\n",
    "Given three random variables $X, Y, B$, then $B$ is a blanket for $X,Y$ iff $X$ and $Y$ are conditionally independent w.r.t. $B$, i.e. $p(X, Y | B) = p(X | B) p(Y | B)$.\n",
    "\n",
    "This concept is essential to be able to define the \"self\", i.e. the system as separated from the outer world. In particular, the action and sensory inputs are the markov blanket for the hidden states of the world and the self."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a3695f-0c55-4022-a1c3-8b0604e9eacc",
   "metadata": {},
   "source": [
    "## Free energy\n",
    "\n",
    "How can we compute the approximateposterior $Q$? Via a free energy minimisation.\n",
    "\n",
    "### Variational free energy\n",
    "\n",
    "We would like to write a funcitonal of $Q$ such that, by minimising it w.r.t. $Q$, then $Q$ is the closest possible to the actual distribution $P$. This will also depend on the sensory input $y$.\n",
    "\n",
    "The minimisation of the free energy will then give us the best $Q$ possible, i.e. the one closest to $P$. However, let's recall that the sentient being is not just about finding $Q$ (this would correspond to the epistemic gain, i.e. optimising your knowledge of the world), but it also has to deal with making actions that keep the sistem in a controlled state! This is the aspect about minimising the surpise.\n",
    "\n",
    "Hence the free energy is:\n",
    "\n",
    "$$F(Q,y) = -\\mathbb E_{Q(x)}(log P(x,y)) - H(Q(x)),$$\n",
    "\n",
    "where the entropy $H$ is \n",
    "\n",
    "$$ H(Q(x)) = \\mathbb E_{Q(x)}(log(Q(x))).$$\n",
    "\n",
    "Recalling that $P(x,y)=P(x|y)P(y)$ by the Bayes theorem, one can manipulate the above to\n",
    "\n",
    "$$F(Q,y) = D_{KL}(P(x|y)||Q(x))-log(P(y)),$$\n",
    "\n",
    "where the $D_{KL}$ is the error between the internal world model and the actual world process (this is about optimising one's beliefs), and the last term $-log(P(y))=log(1/P(y))$ is the surprise (that is optimised via actions).\n",
    "\n",
    "### Expected free energy\n",
    "\n",
    "When **planning** needs to be involved, how can we model it? How can we learn it? Again, minimising the \"expected\" free energy. This is a crucial point: we need to leverage our world model to make prediction of the future and evaluate each policy. Even with coutnerfactuals! If our model is good, then we will get a good policy $\\pi$ that will drive the being behaviour. A policy is a sequence of actions.\n",
    "\n",
    "The energy, being about the future, need to be averaged over all possible outcomes given a policy $\\pi$. Then, we can minimise the energy to obtain the best $\\pi$.\n",
    "\n",
    "The expected free energy is (notice that now both $x$ and $y$ are integrated):\n",
    "\n",
    "$$G(\\pi) = -\\mathbb E_{Q(x,y|\\pi)}(log P(x,y|C)) - H(Q(x|\\pi)) \\ge \\mathbb -E_{Q(x,t | \\pi)}(D_{KL}(Q(x |y, \\pi) || Q(x | \\pi))) + \\mathbb E_{Q(y | \\pi)}(log P(y|C))$$\n",
    "\n",
    "where $Q(x,y|\\pi) := Q(x | \\pi)P(y | x)$ and $C$ are parameters (we will use it to describe the tendency to move towards food and go away from poison). The original definition uses the second expression, not the first one as I did, to define $G(\\pi)$: however, my preference, in analogy with the Variational free energy, is to slightly change the convention.\n",
    "\n",
    "Note that to find $Q$, we need to rely on the minimisation of $F$ and add a conditioning on the policy $\\pi$. So, with a given and fixed $\\pi$:\n",
    "\n",
    "$$ F_\\pi(Q,y) = -\\mathbb E_{Q(x|\\pi)}(log P(x,y|\\pi)) - H(Q(x|\\pi)) $$\n",
    "\n",
    "Notation: in the above equations, $x$ and $y$ are sequences of hidden states and sensory data respectively, of the same time length of the policy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff9c8d0-e6c5-41bf-8e67-441cd949b936",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "**Learning is about doing inference** on the parameters of the distributions involved in the game.\n",
    "\n",
    "This means that the formalism stays the same, except that now some of the distributions will have parameters of which we have a prior belief. Thus, when minimising, we want to also improve those parameters. Usually, in emprical bayes, this is done via the upgrade of the prior distribution $p(\\theta)$ with the posterior $p(\\theta|y)$, where $\\theta$ is a parameter and $y$ the data.\n",
    "\n",
    "You will see, in the example below, that we will be building the world process $P$ using some pre-defined matrices $A$, $B$. Adding priors associated to these matrices in the approximate posterior $Q$, i.e. having $Q$ being a distribution over the space of matrices $A$, $B$... as well, is the first step towards learning. The subsequent step would be to minimise the free energy to find $Q$, including the part over the matrix sapces. We will not add these priors in our computations.\n",
    "\n",
    "## How about attention?\n",
    "\n",
    "Attention is already embedded in this theory, as the way knowledge is chosen to validate our world model world model. Said differently, attention is the process of selecting the highest quality data from what we have already mea­sured and using ­these to validate our world model preditions. The\n",
    "design of the next experiment to ensure the highest quality data is referred to as **saliency**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6782775-4f0d-4e7c-b71e-b3ebfacca3ce",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Let's now build everything in the discrete setting.\n",
    "\n",
    "Our world model will be a POMDP, described by the following bayesian network:\n",
    "\n",
    "![pomdp](pomdp.jpg)\n",
    "\n",
    "This network corresponds to this probability distribution:\n",
    "\n",
    "$$ P(x,y,\\pi,C) = P(C)P(x_0)P(\\pi)\\prod_t^T P(y_t|x_t)P(x_{t+1}|x_t, \\pi_t)$$\n",
    "\n",
    "Also in this case, $x,y$ are sequences of states, depending on $t$!\n",
    "We then have to specify what are the spaces in which the hidden states, the observations etc... belong to.\n",
    "\n",
    "So, let's consider a small agent in a 1D world. The world has **4 spacial sites**, in each of them there can be nothing, food or poison. So $y_t \\in \\{0,1,2\\}$, where `0` is poison, `1` is nothing, `2` is food.\n",
    "\n",
    "The hidden state $x_t$ at time $t$ models the position in the 1D world and is assumed to be a number in $\\{0,1,2,3\\}$.\n",
    "\n",
    "![world](world.jpg)\n",
    "\n",
    "The possible **actions** of the agent are to move right or left . In particular `right = 0, left = 1`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3a3442e-1945-4dda-9ac9-04dcd60b8aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy\n",
      "Q(x|pi)\n",
      "P(y_t|x_t)\n",
      "P(x_0)\n",
      "P(C)\n",
      "P(x_{t+1}|x_t, \\pi_t)\n",
      "G(\\pi)\n",
      "F_\\pi\n",
      "All initialised\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"policy\")\n",
    "pi = [1,1,1,1]  # this is a sequence of actions\n",
    "number_of_policies = 2**4\n",
    "max_time = len(pi)\n",
    "number_of_locations = 4\n",
    "print(\"Q(x|pi)\")  # x = (s0,s1,s2,...,sT)\n",
    "Q = np.random.random((max_time, number_of_locations, number_of_policies))  # we have to find it by minimisation! The dimensions are: \n",
    "# - time (which has dim = 4),\n",
    "# - hidden state space (which has dim = number_of_locations), \n",
    "# - policy (which has dim = number_of_policies)\n",
    "\n",
    "\n",
    "# normalise Q:\n",
    "def normaliseQ():\n",
    "    for t in range(max_time):\n",
    "        for j in range(number_of_policies):\n",
    "            norm = np.sum([Q[t, i, j] for i in range(number_of_locations)])\n",
    "            for i in range(number_of_locations):\n",
    "                if norm == 0:\n",
    "                    raise ValueError(f\"The modelling must be flawed in row {i}\")\n",
    "                Q[t, i, j] = Q[t, i, j]/norm\n",
    "        for i in range(number_of_locations):\n",
    "            for j in range(number_of_policies):\n",
    "                if Q[t, i, j]<=0:\n",
    "                    Q[t, i, j] = 0.0001\n",
    "\n",
    "normaliseQ()  # in place\n",
    "\n",
    "print(\"P(y_t|x_t)\")  # action at each site\n",
    "A = np.array([[0.9,0,0,0.1],[0,1,1,0],[0.1,0,0,0.9]])  # dimensions 3 x number_of_locations\n",
    "\n",
    "print(\"P(x_0)\")  # initial position x_0=1\n",
    "D = np.array([0,1,0,0])  # dimensions number_of_locations\n",
    "\n",
    "print(\"P(C)\")  # this is the preference for food and dislike of poison\n",
    "C = np.array([0.01,0.1,0.1,0.79])  # dimensions number_of_locations\n",
    "\n",
    "print(\"P(x_{t+1}|x_t, \\pi_t)\")  # how to move\n",
    "B = np.zeros([number_of_locations, number_of_locations, 3])  # dimensions: number_of_locations x number_of_locations x 3\n",
    "# eating\n",
    "B[0,0,2] = 1\n",
    "B[1,1,2] = 1\n",
    "B[2,2,2] = 1\n",
    "B[3,3,2] = 1\n",
    "\n",
    "# move right\n",
    "B[1,0,0] = 1\n",
    "B[2,1,0] = 1\n",
    "B[3,2,0] = 1\n",
    "B[3,3,0] = 1\n",
    "\n",
    "# move left\n",
    "B[0,1,1] = 1\n",
    "B[1,2,1] = 1\n",
    "B[2,3,1] = 1\n",
    "B[0,0,1] = 1\n",
    "\n",
    "# normalisation as it is a probability:\n",
    "for j in range(number_of_locations):\n",
    "    for k in range(3):\n",
    "        norm = np.sum([B[i,j,k] for i in range(number_of_locations)])\n",
    "        for i in range(number_of_locations):\n",
    "            if norm == 0:\n",
    "                raise ValueError(f\"The modelling must be flawed in row {i}\")\n",
    "            B[i,j,k] = B[i,j,k]/norm\n",
    "\n",
    "\n",
    "print(\"G(\\pi)\")\n",
    "def G(pi):\n",
    "    \"\"\" expected free energy\n",
    "    pi: policy, a list\n",
    "    \"\"\"\n",
    "    k = np.sum([pi[i]*2**i for i in range(max_time)])\n",
    "    return np.sum([np.sum([-np.log(0.0000001+np.prod([A[j,i]*(D[i] if t == 0 else 1/number_of_locations)*C[i] for t in range(max_time)]))*np.prod([Q[t,i,k]*A[j,i] for t in range(max_time)]) for j in range(3)]) + np.prod([Q[t,i, k] for t in range(max_time)])*np.log(0.0000001+np.prod([Q[t,i, k] for t in range(max_time)])) for i in range(number_of_locations)])  # y -> j, x -> i, pi -> k, time -> t\n",
    "\n",
    "G(pi)\n",
    "print(\"F_\\pi\")\n",
    "\n",
    "def y_from_pi(pi):\n",
    "    \"\"\" get the list of y, given pi\n",
    "    pi: policy, a list\n",
    "    \"\"\"\n",
    "    pos = [1,1,1,1]\n",
    "    for t in range(max_time):\n",
    "        if pi[t] == 0:\n",
    "            for j in range(t,max_time):\n",
    "                pos[j] += 1\n",
    "                pos[j] = min(pos[j],3)\n",
    "        else:\n",
    "            for j in range(t,max_time):\n",
    "                pos[j] -= 1\n",
    "                pos[j] = max(pos[j],0)\n",
    "    #print(pos)\n",
    "    return [0 if pos[t]<=0 else (2 if pos[t] >=3 else 1) for t in range(max_time)]\n",
    "\n",
    "\n",
    "assert y_from_pi([0,0,0,0]) == [1,2,2,2]\n",
    "\n",
    "def F(pi):\n",
    "    \"\"\" variational free energy\n",
    "    pi: policy, a list\n",
    "    y: the list of sensory inputs\n",
    "    \"\"\"\n",
    "    k = np.sum([pi[i]*2**i for i in range(max_time)])\n",
    "    j = y_from_pi(pi)\n",
    "    return np.sum([-np.log(0.0000001+np.prod([A[j[t],i]*B[i,j[t],pi[t]] for t in range(max_time)]))*np.prod([Q[t, i, k] for t in range(max_time)]) + np.prod([Q[t,i, k] for t in range(max_time)])*np.log(0.0000001+np.prod([Q[t,i, k] for t in range(max_time)])) for i in range(number_of_locations)])\n",
    "F(pi)\n",
    "print(\"All initialised\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9869e37-56ba-4406-b7db-c62430ad66a4",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "\n",
    "The next block of code is about finding the optimum $Q$ and optimum $pi$.\n",
    "\n",
    "To do so, we will proceed iteratively, a bit like in the expectation-maximisation problems:\n",
    " - first, given the policy $\\pi$, we minimise $F$ to get the novel $Q$\n",
    " - secondly, we will minimise $G$ to find the optimum policies, and since there are only $3^4$ cases, the optimum will be global\n",
    "\n",
    "The two steps above are iterated a few times, till they converge.\n",
    "\n",
    "### Differentiability\n",
    "\n",
    "We could actually differentiate the variational free energy and get the explicit form of $\\nabla_Q F_\\pi(Q,y)$. Then, once can proceed with the gradient descent method to find the optimal $Q$. We are leaving this step for the interested reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a12962fa-797a-4162-bd67-b37d1a4b1a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_possible_pi = [ele for ele in product(range(2), repeat = 4)]\n",
    "assert len(all_possible_pi) == number_of_policies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "675724a6-bd42-4762-9fac-62d12eecd098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0)\n",
      "(0, 0, 0, 0)\n",
      "(0, 0, 0, 0)\n",
      "(0, 0, 0, 0)\n",
      "(0, 0, 0, 0)\n",
      "(0, 0, 0, 0)\n",
      "(0, 0, 0, 0)\n",
      "(0, 0, 0, 0)\n",
      "(0, 0, 0, 0)\n",
      "(0, 0, 0, 0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_4.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Fs = []\n",
    "MAX_ITER = 10\n",
    "pi_old = pi\n",
    "for _ in range(MAX_ITER):\n",
    "    \n",
    "    # step 1\n",
    "    val_old = F(pi)\n",
    "    for a in range(1000):\n",
    "        Q_old = Q.copy()\n",
    "        Q = Q + 0.01*(np.random.random((max_time, number_of_locations, number_of_policies)) - 0.5)\n",
    "        normaliseQ()\n",
    "        val_new = F(pi)\n",
    "        Fs.append(val_old)\n",
    "        if val_new < val_old:\n",
    "            val_old = val_new\n",
    "        else:\n",
    "            Q = Q_old\n",
    "\n",
    "    # step 2\n",
    "    pos = np.argmin(list(map(G, all_possible_pi)))\n",
    "    pi = all_possible_pi[pos]\n",
    "    #if pi == pi_old:\n",
    "    #    print(\"Done!\")\n",
    "    #    break\n",
    "    pi_old = pi\n",
    "    print(pi)\n",
    "\n",
    "fig = px.scatter(Fs)\n",
    "fig.show(renderer=\"iframe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23cf6d42-97d9-41ce-b143-5e484e07685a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_5.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig = px.imshow(Q[0], labels=dict(x=\"policy\", y=\"x\", color=\"Probability\"),\n",
    "                x=list(map(str, all_possible_pi)),\n",
    "                y=['0', '1', '2', '3'])\n",
    "fig.show(renderer=\"iframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f7b04-8d5f-4fe6-a556-4aa1ed7dea96",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "One expects that $Q$ approximates $P$ and that the chosen policy is an effective one in finding food and moving towards it. So, the optimum policy shall shave more `0`s than `1`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ae18ce-6321-4cd2-83fe-f43da4e64af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
